# Awesome-Video-LLM-Post-Training [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

## Table of Contents

- [Awesome-Video-LLM-Post-Training ](#awesome-video-llm-post-training-)
  - [Table of Contents](#table-of-contents)
  - [Reinforced Video-LLMs](#reinforced-video-llms)
  - [Video-LLM SFT for Reasoning](#video-llm-sft-for-reasoning)
  - [Test-Time Scaling in Video Reasoning](#test-time-scaling-in-video-reasoning)
  - [Benchmarks for Video Reasoning](#benchmarks-for-video-reasoning)
  - [Related Surveys](#related-surveys)
    - [üåü Star History](#-star-history)
    - [üìù Citation](#-citation)

## Reinforced Video-LLMs

| Paper | Link | Code | Dataset | Venue |
| :--- | :---: | :---: | :---: | :---: |
| Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning | [arXiv](https://arxiv.org/abs/2508.04416) | | | |
| ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World Shorts | [arXiv](https://arxiv.org/abs/2507.20939) | | | |
| EmbRACE-3K: Embodied Reasoning and Action in Complex Environments | [arXiv](https://arxiv.org/abs/2507.10548) | | | |
| Scaling RL to Long Videos | [arXiv](https://arxiv.org/abs/2507.07966) | | | |
| Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for Efficient and Enhanced Video Reasoning | [arXiv](https://arxiv.org/abs/2507.06485) | | | |
| Tempo-R0: A Video-MLLM for Temporal Video Grounding through Efficient Temporal Sensing Reinforcement Learning | [arXiv](https://arxiv.org/abs/2507.04702) | | | |
| VRAgent-R1: Boosting Video Recommendation with MLLM-based Agents via Reinforcement Learning | [arXiv](https://arxiv.org/abs/2507.02626) | | | |
| Kwai Keye-VL Technical Report | [arXiv](https://arxiv.org/abs/2507.01949) | | | |
| VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning | [arXiv](https://arxiv.org/abs/2506.17221) | | | |
| Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning | [arXiv](https://arxiv.org/abs/2506.13654) | | | |
| VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks | [arXiv](https://arxiv.org/abs/2506.09079) | | | |
| DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO | [arXiv](https://arxiv.org/abs/2506.07464) | | | |
| AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual Counting for MLLMs | [arXiv](https://arxiv.org/abs/2506.05328) | | | |
| EgoVLM: Policy Optimization for Egocentric Video Understanding | [arXiv](https://arxiv.org/abs/2506.03097) | | | |
| Reinforcement Learning Tuning for VideoLLMs: Reward Design and Data Efficiency | [arXiv](https://arxiv.org/abs/2506.01908) | | | |
| VideoCap-R1: Enhancing MLLMs for Video Captioning via Structured Thinking | [arXiv](https://arxiv.org/abs/2506.01725) | | | |
| ReAgent-V: A Reward-Driven Multi-Agent Framework for Video Understanding | [arXiv](https://arxiv.org/abs/2506.01300) | | | |
| ReFoCUS: Reinforcement-guided Frame Optimization for Contextual Understanding | [arXiv](https://arxiv.org/abs/2506.01274) | | | |
| Reinforcing Video Reasoning with Focused Thinking | [arXiv](https://arxiv.org/abs/2505.24718) | | | |
| VAU-R1: Advancing Video Anomaly Understanding via Reinforcement Fine-Tuning | [arXiv](https://arxiv.org/abs/2505.23504) | | | |
| A2Seek: Towards Reasoning-Centric Benchmark for Aerial Anomaly Understanding | [arXiv](https://arxiv.org/abs/2505.21962) | | | |
| MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware Multi-Segment Grounding | [arXiv](https://arxiv.org/abs/2505.20715) | | | |
| Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration | [arXiv](https://arxiv.org/abs/2505.20256) | | | |
| Vad-R1: Towards Video Anomaly Reasoning via Perception-to-Cognition Chain-of-Thought | [arXiv](https://arxiv.org/abs/2505.19877) | | | |
| VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied Iterative Policy Optimization | [arXiv](https://arxiv.org/abs/2505.19000) | | | |
| Fact-R1: Towards Explainable Video Misinformation Detection with Deep Reasoning | [arXiv](https://arxiv.org/abs/2505.16836) | | | |
| From Evaluation to Defense: Advancing Safety in Video Large Language Models | [arXiv](https://arxiv.org/abs/2505.16643) | | | |
| Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning | [arXiv](https://arxiv.org/abs/2505.15966) | | | |
| ViaRL: Adaptive Temporal Grounding via Visual Iterated Amplification Reinforcement Learning | [arXiv](https://arxiv.org/abs/2505.15447) | | | |
| UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning | [arXiv](https://arxiv.org/abs/2505.14231) | | | |
| BusterX: MLLM-Powered AI-Generated Video Forgery Detection and Explanation | [arXiv](https://arxiv.org/abs/2505.12620) | | | |
| VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning | [arXiv](https://arxiv.org/abs/2505.12434) | | | |
| Seed1.5-VL Technical Report | [arXiv](https://arxiv.org/abs/2505.07062) | | | |
| Compile Scene Graphs with Reinforcement Learning | [arXiv](https://arxiv.org/abs/2504.13617) | | | |
| Self-alignment of Large Video Language Models with Refined Regularized Preference Optimization | [arXiv](https://arxiv.org/abs/2504.12083) | | | |
| Mavors: Multi-granularity Video Representation for Multimodal Large Language Model | [arXiv](https://arxiv.org/abs/2504.10068) | | | |
| TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning | [arXiv](https://arxiv.org/abs/2504.09641) | | | |
| VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning | [arXiv](https://arxiv.org/abs/2504.06958) | | | |
| Spatial-R1: Enhancing MLLMs in Video Spatial Reasoning | [arXiv](https://arxiv.org/abs/2504.01805) | | | |
| Improved Visual-Spatial Reasoning via R1-Zero-Like Training | [arXiv](https://arxiv.org/abs/2504.00883) | | | |
| Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1 | [arXiv](https://arxiv.org/abs/2503.24376) | | | |
| Video-R1: Reinforcing Video Reasoning in MLLMs | [arXiv](https://arxiv.org/abs/2503.21776) | | | |
| Exploring Hallucination of Large Multimodal Models in Video Understanding: Benchmark, Analysis and Mitigation | [arXiv](https://arxiv.org/abs/2503.19622) | | | |
| TimeZero: Temporal Video Grounding with Reasoning-Guided LVLM | [arXiv](https://arxiv.org/abs/2503.13377) | | | |
| ST-Think: How Multimodal Large Language Models Reason About 4D Worlds from Ego-Centric Videos | [arXiv](https://arxiv.org/abs/2503.12542) | | | |
| Memory-enhanced Retrieval Augmentation for Long Video Understanding | [arXiv](https://arxiv.org/abs/2503.09149) | | | |
| video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model | [arXiv](https://arxiv.org/abs/2502.11775) | | | |
| Temporal Preference Optimization for Long-Form Video Understanding | [arXiv](https://arxiv.org/abs/2501.13919) | | | |
| InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model | [arXiv](https://arxiv.org/abs/2501.12368) | | | |
| VidChain: Chain-of-Tasks with Metric-based Direct Preference Optimization for Dense Video Captioning | [arXiv](https://arxiv.org/abs/2501.06761) | | | |
| VideoSAVi: Self-Aligned Video Language Models without Human Supervision | [arXiv](https://arxiv.org/abs/2412.00624) | | | |


## Video-LLM SFT for Reasoning

| Paper | Link | Code | Dataset | Venue |
| :--- | :---: | :---: | :---: | :---: |
| Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning | [arXiv](https://arxiv.org/abs/2508.04416) | | | |
| ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World Shorts | [arXiv](https://arxiv.org/abs/2507.20939) | | | |
| CoTasks: Chain-of-Thought based Video Instruction Tuning Tasks | [arXiv](https://arxiv.org/abs/2507.13609) | | | |
| EmbRACE-3K: Embodied Reasoning and Action in Complex Environments | [arXiv](https://arxiv.org/abs/2507.10548) | | | |
| Scaling RL to Long Videos | [arXiv](https://arxiv.org/abs/2507.07966) | | | |
| Video Event Reasoning and Prediction by Fusing World Knowledge from LLMs with Vision Foundation Models | [arXiv](https://arxiv.org/abs/2507.05822) | | | |
| Kwai Keye-VL Technical Report | [arXiv](https://arxiv.org/abs/2507.01949) | | | |
| VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning | [arXiv](https://arxiv.org/abs/2506.17221) | | | |
| Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning | [arXiv](https://arxiv.org/abs/2506.13654) | | | |
| DAVID-XR1: Detecting AI-Generated Videos with Explainable Reasoning | [arXiv](https://arxiv.org/abs/2506.14827) | | | |
| VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks | [arXiv](https://arxiv.org/abs/2506.09079) | | | |
| AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual Counting for MLLMs | [arXiv](https://arxiv.org/abs/2506.05328) | | | |
| Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video Reasoning | [arXiv](https://arxiv.org/abs/2506.03525) | | | |
| ReAgent-V: A Reward-Driven Multi-Agent Framework for Video Understanding | [arXiv](https://arxiv.org/abs/2506.01300) | | | |
| Chain-of-Frames: Advancing Video Understanding in Multimodal LLMs via Frame-Aware Reasoning | [arXiv](https://arxiv.org/abs/2506.00318) | | | |
| Universal Visuo-Tactile Video Understanding for Embodied Interaction | [arXiv](https://arxiv.org/abs/2505.22566) | | | |
| Fostering Video Reasoning via Next-Event Prediction | [arXiv](https://arxiv.org/abs/2505.22457) | | | |
| A2Seek: Towards Reasoning-Centric Benchmark for Aerial Anomaly Understanding | [arXiv](https://arxiv.org/abs/2505.21962) | | | |
| Vad-R1: Towards Video Anomaly Reasoning via Perception-to-Cognition Chain-of-Thought | [arXiv](https://arxiv.org/abs/2505.19877) | | | |
| Fact-R1: Towards Explainable Video Misinformation Detection with Deep Reasoning | [arXiv](https://arxiv.org/abs/2505.16836) | | | |
| From Evaluation to Defense: Advancing Safety in Video Large Language Models | [arXiv](https://arxiv.org/abs/2505.16643) | | | |
| Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning | [arXiv](https://arxiv.org/abs/2505.15966) | | | |
| UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning | [arXiv](https://arxiv.org/abs/2505.14231) | | | |
| VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning | [arXiv](https://arxiv.org/abs/2505.12434) | | | |
| Seed1.5-VL Technical Report | [arXiv](https://arxiv.org/abs/2505.07062) | | | |
| TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action | [arXiv](https://arxiv.org/abs/2505.01583) | | | |
| VEU-Bench: Towards Comprehensive Understanding of Video Editing | [arXiv](https://arxiv.org/abs/2504.17828) | | | |
| Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models | [arXiv](https://arxiv.org/abs/2504.15271) | | | |
| Compile Scene Graphs with Reinforcement Learning | [arXiv](https://arxiv.org/abs/2504.13617) | | | |
| Mavors: Multi-granularity Video Representation for Multimodal Large Language Model | [arXiv](https://arxiv.org/abs/2504.10068) | | | |
| LVC: A Lightweight Compression Framework for Enhancing VLMs in Long Video Understanding | [arXiv](https://arxiv.org/abs/2504.06835) | | | |
| From 128K to 4M: Efficient Training of Ultra-Long Context Large Language Models | [arXiv](https://arxiv.org/abs/2504.06214) | | | |
| Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1 | [arXiv](https://arxiv.org/abs/2503.24376) | | | |
| Video-R1: Reinforcing Video Reasoning in MLLMs | [arXiv](https://arxiv.org/abs/2503.21776) | | | |
| PAVE: Patching and Adapting Video Large Language Models | [arXiv](https://arxiv.org/abs/2503.19794) | | | |
| Exploring Hallucination of Large Multimodal Models in Video Understanding: Benchmark, Analysis and Mitigation | [arXiv](https://arxiv.org/abs/2503.19622) | | | |
| VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning | [arXiv](https://arxiv.org/abs/2503.13444) | | | |
| ST-Think: How Multimodal Large Language Models Reason About 4D Worlds from Ego-Centric Videos | [arXiv](https://arxiv.org/abs/2503.12542) | | | |
| TIME: Temporal-sensitive Multi-dimensional Instruction Tuning and Benchmarking for Video-LLMs | [arXiv](https://arxiv.org/abs/2503.09994) | | | |
| Memory-enhanced Retrieval Augmentation for Long Video Understanding | [arXiv](https://arxiv.org/abs/2503.09149) | | | |
| UrbanVideo-Bench: Benchmarking Vision-Language Models on Embodied Intelligence with Video Data in Urban Spaces | [arXiv](https://arxiv.org/abs/2503.06157) | | | |
| Token-Efficient Long Video Understanding for Multimodal LLMs | [arXiv](https://arxiv.org/abs/2503.04130) | | | |
| M-LLM Based Video Frame Selection for Efficient Video Understanding | [arXiv](https://arxiv.org/abs/2502.19680) | | | |
| video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model | [arXiv](https://arxiv.org/abs/2502.11775) | | | |
| Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with Leading Short-Context Accuray | [arXiv](https://arxiv.org/abs/2502.05177) | | | |
| InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model | [arXiv](https://arxiv.org/abs/2501.12368) | | | |
| Omni-RGPT: Unifying Image and Video Region-level Understanding via Token Marks | [arXiv](https://arxiv.org/abs/2501.08326) | | | |
| LongViTU: Instruction Tuning for Long-Form Video Understanding | [arXiv](https://arxiv.org/abs/2501.05037) | | | |
| VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM | [arXiv](https://arxiv.org/abs/2501.00599) | | | |
| Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces | [arXiv](https://arxiv.org/abs/2412.14171) | | | |
| Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling | [arXiv](https://arxiv.org/abs/2412.05271) | | | |
| STEP: Enhancing Video-LLMs' Compositional Reasoning by Spatio-Temporal Graph-guided Self-Training | [arXiv](https://arxiv.org/abs/2412.00161) | | | |
| ReVisionLLM: Recursive Vision-Language Model for Temporal Grounding in Hour-Long Videos | [arXiv](https://arxiv.org/abs/2411.14901) | | | |
| VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection | [arXiv](https://arxiv.org/abs/2411.14794) | | | |
| Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models | [arXiv](https://arxiv.org/abs/2410.03290) | | | |


## Test-Time Scaling in Video Reasoning

| Paper | Link | Code | Dataset | Venue |
| :--- | :---: | :---: | :---: | :---: |
| Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning | [arXiv](https://arxiv.org/abs/2508.04416) | | | |
| VideoForest: Person-Anchored Hierarchical Reasoning for Cross-Video Question Answering | [arXiv](https://arxiv.org/abs/2508.03039) | | | |
| Free-MoRef: Instantly Multiplexing Context Perception Capabilities of Video-MLLMs within Single Inference | [arXiv](https://arxiv.org/abs/2508.02134) | | | |
| EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent | [arXiv](https://arxiv.org/abs/2507.15428) | | | |
| Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding | [arXiv](https://arxiv.org/abs/2507.15028) | | | |
| ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models | [arXiv](https://arxiv.org/abs/2507.09876) | | | |
| Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for Efficient and Enhanced Video Reasoning | [arXiv](https://arxiv.org/abs/2507.06485) | | | |
| StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context Modeling | [arXiv](https://arxiv.org/abs/2507.05240) | | | |
| VRAgent-R1: Boosting Video Recommendation with MLLM-based Agents via Reinforcement Learning | [arXiv](https://arxiv.org/abs/2507.02626) | | | |
| DIVE: Deep-search Iterative Video Exploration A Technical Report for the CVRR Challenge at CVPR 2025 | [arXiv](https://arxiv.org/abs/2506.21891) | | | |
| How Far Can Off-the-Shelf Multimodal Large Language Models Go in Online Episodic Memory Question Answering? | [arXiv](https://arxiv.org/abs/2506.16450) | | | |
| Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning | [arXiv](https://arxiv.org/abs/2506.13654) | | | |
| VideoDeepResearch: Long Video Understanding With Agentic Tool Using | [arXiv](https://arxiv.org/abs/2506.10821) | | | |
| CogStream: Context-guided Streaming Video Question Answering | [arXiv](https://arxiv.org/abs/2506.10516) | | | |
| Wait, We Don't Need to "Wait"! Removing Thinking Tokens Improves Reasoning Efficiency | [arXiv](https://arxiv.org/abs/2506.08343) | | | |
| CyberV: Cybernetics for Test-time Scaling in Video Understanding | [arXiv](https://arxiv.org/abs/2506.07971) | | | |
| Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs | [arXiv](https://arxiv.org/abs/2506.07180) | | | |
| VideoChat-A1: Thinking with Long Videos by Chain-of-Shot Reasoning | [arXiv](https://arxiv.org/abs/2506.06097) | | | |
| ReAgent-V: A Reward-Driven Multi-Agent Framework for Video Understanding | [arXiv](https://arxiv.org/abs/2506.01300) | | | |
| ReFoCUS: Reinforcement-guided Frame Optimization for Contextual Understanding | [arXiv](https://arxiv.org/abs/2506.01274) | | | |
| SiLVR: A Simple Language-based Video Reasoning Framework | [arXiv](https://arxiv.org/abs/2505.24869) | | | |
| Multi-RAG: A Multimodal Retrieval-Augmented Generation System for Adaptive Video Understanding | [arXiv](https://arxiv.org/abs/2505.23990) | | | |
| VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video Reasoning? | [arXiv](https://arxiv.org/abs/2505.23359) | | | |
| Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration | [arXiv](https://arxiv.org/abs/2505.20256) | | | |
| Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding | [arXiv](https://arxiv.org/abs/2505.18079) | | | |
| Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning | [arXiv](https://arxiv.org/abs/2505.15966) | | | |
| ViQAgent: Zero-Shot Video Question Answering via Agent with Open-Vocabulary Grounding Validation | [arXiv](https://arxiv.org/abs/2505.15928) | | | |
| ViaRL: Adaptive Temporal Grounding via Visual Iterated Amplification Reinforcement Learning | [arXiv](https://arxiv.org/abs/2505.15447) | | | |
| RVTBench: A Benchmark for Visual Reasoning Tasks | [arXiv](https://arxiv.org/abs/2505.11838) | | | |
| CoT-Vid: Dynamic Chain-of-Thought Routing with Self Verification for Training-Free Video Reasoning | [arXiv](https://arxiv.org/abs/2505.11830) | | | |
| VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large Video Language Models | [arXiv](https://arxiv.org/abs/2505.08455) | | | |
| Seed1.5-VL Technical Report | [arXiv](https://arxiv.org/abs/2505.07062) | | | |
| Empowering Agentic Video Analytics Systems with Video Language Models | [arXiv](https://arxiv.org/abs/2505.00254) | | | |
| SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding | [arXiv](https://arxiv.org/abs/2504.21435) | | | |
| VideoMultiAgents: A Multi-Agent Framework for Video Question Answering | [arXiv](https://arxiv.org/abs/2504.20091) | | | |
| MR. Video: "MapReduce" is the Principle for Long Video Understanding | [arXiv](https://arxiv.org/abs/2504.16082) | | | |
| Multimodal Long Video Modeling Based on Temporal Dynamic Context | [arXiv](https://arxiv.org/abs/2504.10443) | | | |
| VideoAgent2: Enhancing the LLM-Based Agent System for Long-Form Video Understanding by Uncertainty-Aware CoT | [arXiv](https://arxiv.org/abs/2504.04471) | | | |
| WikiVideo: Article Generation from Multiple Videos | [arXiv](https://arxiv.org/abs/2504.00939) | | | |
| Aurelia: Test-time Reasoning Distillation in Audio-Visual LLMs | [arXiv](https://arxiv.org/abs/2503.23219) | | | |
| Online Reasoning Video Segmentation with Just-in-Time Digital Twins | [arXiv](https://arxiv.org/abs/2503.21056) | | | |
| From Trial to Triumph: Advancing Long Video Understanding via Visual Context Sample Scaling and Self-reward Alignment | [arXiv](https://arxiv.org/abs/2503.20472) | | | |
| Agentic Keyframe Search for Video Question Answering | [arXiv](https://arxiv.org/abs/2503.16032) | | | |
| VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning | [arXiv](https://arxiv.org/abs/2503.13444) | | | |
| Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma? | [arXiv](https://arxiv.org/abs/2503.12496) | | | |
| Memory-enhanced Retrieval Augmentation for Long Video Understanding | [arXiv](https://arxiv.org/abs/2503.09149) | | | |
| Everything Can Be Described in Words: A Simple Unified Multi-Modal Framework with Semantic and Temporal Alignment | [arXiv](https://arxiv.org/abs/2503.09081) | | | |
| QuoTA: Query-oriented Token Assignment via CoT Query Decouple for Long Video Comprehension | [arXiv](https://arxiv.org/abs/2503.08689) | | | |
| Token-Efficient Long Video Understanding for Multimodal LLMs | [arXiv](https://arxiv.org/abs/2503.04130) | | | |
| M-LLM Based Video Frame Selection for Efficient Video Understanding | [arXiv](https://arxiv.org/abs/2502.19680) | | | |
| TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem Understanding | [arXiv](https://arxiv.org/abs/2502.19400) | | | |
| CoS: Chain-of-Shot Prompting for Long Video Understanding | [arXiv](https://arxiv.org/abs/2502.06428) | | | |
| Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with Leading Short-Context Accuray | [arXiv](https://arxiv.org/abs/2502.05177) | | | |
| Streaming Video Understanding and Multi-round Interaction with Memory-enhanced Knowledge | [arXiv](https://arxiv.org/abs/2501.13468) | | | |
| InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model | [arXiv](https://arxiv.org/abs/2501.12368) | | | |
| The Devil is in Temporal Token: High Quality Video Reasoning Segmentation | [arXiv](https://arxiv.org/abs/2501.08549) | | | |
| MECD+: Unlocking Event-Level Causal Graph Discovery for Video Reasoning | [arXiv](https://arxiv.org/abs/2501.07227) | | | |
| VidChain: Chain-of-Tasks with Metric-based Direct Preference Optimization for Dense Video Captioning | [arXiv](https://arxiv.org/abs/2501.06761) | | | |
| Building a Mind Palace: Structuring Environment-Grounded Semantic Graphs for Effective Long Video Analysis with LLMs | [arXiv](https://arxiv.org/abs/2501.04336) | | | |
| Embodied VideoAgent: Persistent Memory from Egocentric Videos and Embodied Sensors Enables Dynamic Scene Understanding | [arXiv](https://arxiv.org/abs/2501.00358) | | | |
| PruneVid: Visual Token Pruning for Efficient Video Large Language Models | [arXiv](https://arxiv.org/abs/2412.16117) | | | |
| Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces | [arXiv](https://arxiv.org/abs/2412.14171) | | | |
| VCA: Video Curious Agent for Long Video Understanding | [arXiv](https://arxiv.org/abs/2412.10471) | | | |
| Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling | [arXiv](https://arxiv.org/abs/2412.05271) | | | |
| VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding | [arXiv](https://arxiv.org/abs/2412.03735) | | | |
| VideoICL: Confidence-based Iterative In-context Learning for Out-of-Distribution Video Understanding | [arXiv](https://arxiv.org/abs/2412.02186) | | | |
| ReVisionLLM: Recursive Vision-Language Model for Temporal Grounding in Hour-Long Videos | [arXiv](https://arxiv.org/abs/2411.14901) | | | |
| VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection | [arXiv](https://arxiv.org/abs/2411.14794) | | | |
| Adaptive Video Understanding Agent: Enhancing efficiency with dynamic frame sampling and feedback-driven reasoning | [arXiv](https://arxiv.org/abs/2410.20252) | | | |
| VideoINSTA: Zero-shot Long Video Understanding via Informative Spatial-Temporal Reasoning with LLMs | [arXiv](https://arxiv.org/abs/2409.20365) | | | |
| MECD: Unlocking Multi-Event Causal Discovery in Video Reasoning | [arXiv](https://arxiv.org/abs/2409.17647) | | | |
| AMEGO: Active Memory from long EGOcentric videos | [arXiv](https://arxiv.org/abs/2409.10917) | | | |
| Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition | [arXiv](https://arxiv.org/abs/2501.03230) | | | |


## Benchmarks for Video Reasoning

| Paper | Link | Code | Dataset | Venue |
| :--- | :---: | :---: | :---: | :---: |
| Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding | [arXiv](https://arxiv.org/abs/2507.15028) | | | |
| HV-MMBench: Benchmarking MLLMs for Human-Centric Video Understanding | [arXiv](https://arxiv.org/abs/2507.04909) | | | |
| ImplicitQA: Going beyond frames towards Implicit Video Reasoning | [arXiv](https://arxiv.org/abs/2506.21742) | | | |
| Looking Beyond Visible Cues: Implicit Video Question Answering via Dual-Clue Reasoning | [arXiv](https://arxiv.org/abs/2506.07811) | | | |
| MORSE-500: A Programmatically Controllable Video Benchmark to Stress-Test Multimodal Reasoning | [arXiv](https://arxiv.org/abs/2506.05523) | | | |
| Go Beyond Earth: Understanding Human Actions and Scenes in Microgravity Environments | [arXiv](https://arxiv.org/abs/2506.02845) | | | |
| Time Blindness: Why Video-Language Models Can't See What Humans Can? | [arXiv](https://arxiv.org/abs/2505.24867) | | | |
| ScaleLong: A Multi-Timescale Benchmark for Long Video Understanding | [arXiv](https://arxiv.org/abs/2505.23922) | | | |
| VidText: Towards Comprehensive Evaluation for Video Text Understanding | [arXiv](https://arxiv.org/abs/2505.22810) | | | |
| Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning? | [arXiv](https://arxiv.org/abs/2505.21374) | | | |
| VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation | [arXiv](https://arxiv.org/abs/2505.14640) | | | |
| Breaking Down Video LLM Benchmarks: Knowledge, Spatial Perception, or True Temporal Understanding? | [arXiv](https://arxiv.org/abs/2505.14321) | | | |
| MINERVA: Evaluating Complex Video Reasoning | [arXiv](https://arxiv.org/abs/2505.00681) | | | |
| IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs | [arXiv](https://arxiv.org/abs/2504.15415) | | | |
| VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning | [arXiv](https://arxiv.org/abs/2504.07956) | | | |
| InstructionBench: An Instructional Video Understanding Benchmark | [arXiv](https://arxiv.org/abs/2504.05040) | | | |
| H2VU-Benchmark: A Comprehensive Benchmark for Hierarchical Holistic Video Understanding | [arXiv](https://arxiv.org/abs/2503.24008) | | | |
| OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts | [arXiv](https://arxiv.org/abs/2503.22952) | | | |
| V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning | [arXiv](https://arxiv.org/abs/2503.11495) | | | |
| Reasoning is All You Need for Video Generalization: A Counterfactual Benchmark with Sub-question Evaluation | [arXiv](https://arxiv.org/abs/2503.10691) | | | |
| Towards Fine-Grained Video Question Answering | [arXiv](https://arxiv.org/abs/2503.06820) | | | |
| SVBench: A Benchmark with Temporal Multi-Turn Dialogues for Streaming Video Understanding | [arXiv](https://arxiv.org/abs/2502.10810) | | | |
| MMVU: Measuring Expert-Level Multi-Discipline Video Understanding | [arXiv](https://arxiv.org/abs/2501.12380) | | | |
| OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding? | [arXiv](https://arxiv.org/abs/2501.05510) | | | |
| HLV-1K: A Large-scale Hour-Long Video Benchmark for Time-Specific Long Video Understanding | [arXiv](https://arxiv.org/abs/2501.01645) | | | |
| Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces | [arXiv](https://arxiv.org/abs/2412.14171) | | | |
| Neptune: The Long Orbit to Benchmarking Long Video Understanding | [arXiv](https://arxiv.org/abs/2412.09582) | | | |
| 3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark | [arXiv](https://arxiv.org/abs/2412.07825) | | | |
| Black Swan: Abductive and Defeasible Video Reasoning in Unpredictable Events | [arXiv](https://arxiv.org/abs/2412.05725) | | | |
| VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding | [arXiv](https://arxiv.org/abs/2412.03735) | | | |
| Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level | [arXiv](https://arxiv.org/abs/2411.09921) | | | |
| TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models | [arXiv](https://arxiv.org/abs/2410.23266) | | | |
| TemporalBench: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models | [arXiv](https://arxiv.org/abs/2410.10818) | | | |
| One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos | [arXiv](https://arxiv.org/abs/2409.19603) | | | |
| Compositional Physical Reasoning of Objects and Events from Videos | [arXiv](https://arxiv.org/abs/2408.02687) | | | |
| ViLLa: Video Reasoning Segmentation with Large Language Model | [arXiv](https://arxiv.org/abs/2407.14500) | | | |


## Related Surveys

| Paper | Link | Code | Dataset | Venue |
| :--- | :---: | :---: | :---: | :---: |
| Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models | [arXiv](https://arxiv.org/abs/2505.04921) | | | |
| VideoLLM Benchmarks and Evaluation: A Survey | [arXiv](https://arxiv.org/abs/2505.03829) | | | |
| Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models | [arXiv](https://arxiv.org/abs/2504.21277) | | | |
| Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey | [arXiv](https://arxiv.org/abs/2503.12605) | | | |
| From Seconds to Hours: Reviewing MultiModal Large Language Models on Comprehensive Long Video Understanding | [arXiv](https://arxiv.org/abs/2409.18938) | | | |
| From Seconds to Hours: Reviewing MultiModal Large Language Models on Comprehensive Long Video Understanding | [arXiv](https://arxiv.org/abs/2409.18938) | | | |
| VideoQA in the Era of LLMs: An Empirical Study | [arXiv](https://arxiv.org/abs/2408.04223) | | | |
| Video Understanding with Large Language Models: A Survey | [arXiv](https://arxiv.org/abs/2312.17322) | | | |

### üåü Star History

[![Star History Chart](https://api.star-history.com/svg?repos=yunlong10/Awesome-Video-LLM-Post-Training&type=Date)](https://star-history.com/#yunlong10/Awesome-Video-LLM-Post-Training&Date)


### üìù Citation

```bibtex
@misc{tang2025videollmposttraining,
  title={Awesome Video-LLM Post-Training},
  year={2025},
  publisher={GitHub},
  howpublished={\url{https://github.com/yunlong10/Awesome-Video-LLM-Post-Training}},
}
```